{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"amba-analysis-worker-discussion The discussion worker, more accurately the Twitter worker, is a component that transforms event data to processed data, especially the score allowing to rank and qualify the tweet itself. Therefore, tweet features that allow scoring need to be extracted from the tweet, and a total score has to be calculated based on all features. The developed scoring mechanism allows representing the impact of an event on the discussion of a publication and, therefore, the publication itself. It is implemented in python using the amba-event-streams package. Hashtags, Entities, Author Name and Location, and \u201ctop used tweet words\u201d are extracted from a tweet but are not used to generate a score (since this data is interesting to be collected but does not qualify the tweet at all). Author Location extracts the location data from the Twitter supplied author data. This location data is a user-defined string that is geo-encoded using a free service[^1]. The total tweet score is a weighted sum of part scores multiplied by a factor generated by its type. While not changing the content of a tweet, the tweet type is a significant indicator of its impact. While a retweet is the fastest way to tweet, it does not add personal options and is much less likely to be seen by people in their timeline, thus resulting in the lowest possible factor. On the other hand, a quoted tweet and a response have a bigger chance to be seen and add value to the discussion and therefore resulting in a higher factor. Finally, an original tweet is the highest factor since it is likely to start a discussion. The different types and their respective factors can be seen in Table 1 . The second-biggest factor in the impact of a tweet, and therefore the highest weighted, is the tweet author itself. The number of followers that can see the tweet, whether they are verified or not, and the bot detection is significant for the author\u2019s scoring. Details to each of the individual scoring can be seen in Table 2 . | Type Factor | | Score Abstract Similarity | | Score Sentiment | | Followers | |------------:|----:|--------------------------:|:----|----------------:|----:|-------------------------------------------:| | quoted | 0.6 | \\>0.9 | 3 | \\>0.6 | 10 | log 2 *f**o**l**l**o**w**e**r**s* | | replied_to | 0.7 | \\>0.8 | 5 | \\>0.33 | 9 | | | retweet | 0.1 | \\>0.5 | 10 | \\>0.1 | 7 | | | tweet | 1 | \\>0.2 | 3 | \\ < -0.1 | 2 | | | | | else | 1 | \\ < -0.33 | 1 | | | | | | | \\ < -0.6 | 0 | | | | | | | else | 5 | | Meta Score Calculation The tweet content is essential for scoring as well. In order to generate a content score, the length, sentiment, and percentage of abstract matching of the tweet are considered. Sentiment and Abstract similarity are calculated using the Spacy framework with eight language packages (de, es, en, fr, ja, it, ru, pl). In the case of unknown languages, neutral values are returned. Further text preprocessing is done to improve results and performance. Therefore, all stop words, short words with less than three letters, URLs, and words neither a Noun, Propn (proper Noun), or Verb are removed. The sentiment varying between 1 (positive) and -1 (negative) is linearly in buckets over proportional, favoring a positive sentiment. The abstract similarity is a bit more complicated to score. While a high value is bad since it is not adding anything, a low value indicates that the tweet content is likely not about the publication content. The length scoring bucketing is in three main buckets; one is just a link or a few words, the second is a sentence max, and the last requires a few words. Exact values can be seen in Table seen in Table 1 . | Score Length | | Score Time | Score Bot | | Score Verified | | |-------------:|----:|--------------------------------:|----------:|----:|---------------:|----:| | \\ < 50 | 3 | log\u2006(*X*))/log\u2006(1/7)\u2005+\u20053)\u2005\\*\u200510 | no Bot | 10 | verified | 10 | | \\ < 100 | 6 | score \\ < = 30 | Bot | 1 | not verified | 5 | | else 100 | 10 | score \\>= 1 | | | | | Content Score Calculation Furthermore, a score is calculated based on the time that has passed since the publication was published. The score is based on studies showing the importance of early sharing increasing the citation count in the future. The most crucial time range is a week. The formula used can be seen in Table 2 . The values are limited in both directions and will always be between 1 and 30. Additionally, it is helping to highlight new research. Its weight to the overall score is relatively low. Finally, the following weighted sum with the following weights is used to calculate the total score of the tweet. score\u2004=\u2004type factor \u2005\\\u2005( 3 * score time + 6 * score user + 5 * score content ) The score, the extracted data, and features are then stored in the event, which after a status update to processed will be sent to Kafka. [^1]: https://nominatim.openstreetmap.org ; accessed 30-October-2021","title":"Home"},{"location":"#amba-analysis-worker-discussion","text":"The discussion worker, more accurately the Twitter worker, is a component that transforms event data to processed data, especially the score allowing to rank and qualify the tweet itself. Therefore, tweet features that allow scoring need to be extracted from the tweet, and a total score has to be calculated based on all features. The developed scoring mechanism allows representing the impact of an event on the discussion of a publication and, therefore, the publication itself. It is implemented in python using the amba-event-streams package. Hashtags, Entities, Author Name and Location, and \u201ctop used tweet words\u201d are extracted from a tweet but are not used to generate a score (since this data is interesting to be collected but does not qualify the tweet at all). Author Location extracts the location data from the Twitter supplied author data. This location data is a user-defined string that is geo-encoded using a free service[^1]. The total tweet score is a weighted sum of part scores multiplied by a factor generated by its type. While not changing the content of a tweet, the tweet type is a significant indicator of its impact. While a retweet is the fastest way to tweet, it does not add personal options and is much less likely to be seen by people in their timeline, thus resulting in the lowest possible factor. On the other hand, a quoted tweet and a response have a bigger chance to be seen and add value to the discussion and therefore resulting in a higher factor. Finally, an original tweet is the highest factor since it is likely to start a discussion. The different types and their respective factors can be seen in Table 1 . The second-biggest factor in the impact of a tweet, and therefore the highest weighted, is the tweet author itself. The number of followers that can see the tweet, whether they are verified or not, and the bot detection is significant for the author\u2019s scoring. Details to each of the individual scoring can be seen in Table 2 . | Type Factor | | Score Abstract Similarity | | Score Sentiment | | Followers | |------------:|----:|--------------------------:|:----|----------------:|----:|-------------------------------------------:| | quoted | 0.6 | \\>0.9 | 3 | \\>0.6 | 10 | log 2 *f**o**l**l**o**w**e**r**s* | | replied_to | 0.7 | \\>0.8 | 5 | \\>0.33 | 9 | | | retweet | 0.1 | \\>0.5 | 10 | \\>0.1 | 7 | | | tweet | 1 | \\>0.2 | 3 | \\ < -0.1 | 2 | | | | | else | 1 | \\ < -0.33 | 1 | | | | | | | \\ < -0.6 | 0 | | | | | | | else | 5 | | Meta Score Calculation The tweet content is essential for scoring as well. In order to generate a content score, the length, sentiment, and percentage of abstract matching of the tweet are considered. Sentiment and Abstract similarity are calculated using the Spacy framework with eight language packages (de, es, en, fr, ja, it, ru, pl). In the case of unknown languages, neutral values are returned. Further text preprocessing is done to improve results and performance. Therefore, all stop words, short words with less than three letters, URLs, and words neither a Noun, Propn (proper Noun), or Verb are removed. The sentiment varying between 1 (positive) and -1 (negative) is linearly in buckets over proportional, favoring a positive sentiment. The abstract similarity is a bit more complicated to score. While a high value is bad since it is not adding anything, a low value indicates that the tweet content is likely not about the publication content. The length scoring bucketing is in three main buckets; one is just a link or a few words, the second is a sentence max, and the last requires a few words. Exact values can be seen in Table seen in Table 1 . | Score Length | | Score Time | Score Bot | | Score Verified | | |-------------:|----:|--------------------------------:|----------:|----:|---------------:|----:| | \\ < 50 | 3 | log\u2006(*X*))/log\u2006(1/7)\u2005+\u20053)\u2005\\*\u200510 | no Bot | 10 | verified | 10 | | \\ < 100 | 6 | score \\ < = 30 | Bot | 1 | not verified | 5 | | else 100 | 10 | score \\>= 1 | | | | | Content Score Calculation Furthermore, a score is calculated based on the time that has passed since the publication was published. The score is based on studies showing the importance of early sharing increasing the citation count in the future. The most crucial time range is a week. The formula used can be seen in Table 2 . The values are limited in both directions and will always be between 1 and 30. Additionally, it is helping to highlight new research. Its weight to the overall score is relatively low. Finally, the following weighted sum with the following weights is used to calculate the total score of the tweet. score\u2004=\u2004type factor \u2005\\\u2005( 3 * score time + 6 * score user + 5 * score content ) The score, the extracted data, and features are then stored in the event, which after a status update to processed will be sent to Kafka. [^1]: https://nominatim.openstreetmap.org ; accessed 30-October-2021","title":"amba-analysis-worker-discussion"},{"location":"twitter_worker_ref/","text":"TwitterWorker ( EventStreamConsumer , EventStreamProducer ) process tweets get_author_data ( tweet_data ) staticmethod get the author data of a tweet Parameters: Name Type Description Default tweet_data the tweet data we want an author from required Source code in src/twitter_worker.py @staticmethod def get_author_data ( tweet_data ): \"\"\"get the author data of a tweet Arguments: tweet_data: the tweet data we want an author from \"\"\" author_id = tweet_data [ 'author_id' ] for user in tweet_data [ 'includes' ][ 'users' ]: if user [ 'id' ] == author_id : return user return None normalize_abstract_value ( value ) staticmethod normalize the calculated value from an abstract comparison Parameters: Name Type Description Default value the value to be normalized required Source code in src/twitter_worker.py @staticmethod def normalize_abstract_value ( value ): \"\"\"normalize the calculated value from an abstract comparison Arguments: value: the value to be normalized \"\"\" if value > 0.9 : return 3 if value > 0.8 : return 5 if value > 0.5 : return 10 if value > 0.2 : return 3 return 1 normalize_sentiment_value ( value ) staticmethod normalize the calculated value from an sentiment comparison better sentiment -> better value Parameters: Name Type Description Default value the value to be normalized required Source code in src/twitter_worker.py @staticmethod def normalize_sentiment_value ( value ): \"\"\"normalize the calculated value from an sentiment comparison better sentiment -> better value Arguments: value: the value to be normalized \"\"\" if value > 0.6 : return 10 if value > 0.33 : return 9 if value > 0.1 : return 7 if value < - 0.6 : return 0 if value < - 0.33 : return 1 if value < - 0.1 : return 2 return 5 on_message ( self , json_msg ) process a tweet Parameters: Name Type Description Default json_msg the json_msg containing the event to be processed required Source code in src/twitter_worker.py def on_message ( self , json_msg ): \"\"\"process a tweet Arguments: json_msg: the json_msg containing the event to be processed \"\"\" if not self . dao : self . dao = DAO () logging . warning ( self . log + \" create dao\" ) logging . info ( self . log + \"on message twitter consumer\" ) e = Event () e . from_json ( json_msg ) if e . get ( 'source_id' ) == 'twitter' : e . data [ 'subj' ][ 'processed' ] = {} e . data [ 'subj' ][ 'processed' ][ 'question_mark_count' ] = e . data [ 'subj' ][ 'data' ][ 'text' ] . count ( \"?\" ) e . data [ 'subj' ][ 'processed' ][ 'exclamation_mark_count' ] = e . data [ 'subj' ][ 'data' ][ 'text' ] . count ( \"!\" ) e . data [ 'subj' ][ 'processed' ][ 'length' ] = len ( e . data [ 'subj' ][ 'data' ][ 'text' ]) pub_timestamp = date ( 2012 , 1 , 1 ) if 'year' in e . data [ 'obj' ][ 'data' ]: pub_timestamp = date ( int ( e . data [ 'obj' ][ 'data' ][ 'year' ]), 1 , 1 ) if 'pub_date' in e . data [ 'obj' ][ 'data' ] and e . data [ 'obj' ][ 'data' ][ 'pub_date' ]: split_date = e . data [ 'obj' ][ 'data' ][ 'pub_date' ] . split ( '-' ) if len ( split_date ) > 2 : pub_timestamp = date ( int ( split_date [ 0 ]), int ( split_date [ 1 ]), int ( split_date [ 2 ])) e . data [ 'subj' ][ 'processed' ][ 'time_past' ] = ( date . today () - pub_timestamp ) . days hashtags = [] annotations = [] a_types = [] if 'entities' in e . data [ 'subj' ][ 'data' ]: if 'hashtags' in e . data [ 'subj' ][ 'data' ][ 'entities' ]: for tag in e . data [ 'subj' ][ 'data' ][ 'entities' ][ 'hashtags' ]: hashtags . append ( normalize ( tag [ 'tag' ])) if 'annotations' in e . data [ 'subj' ][ 'data' ][ 'entities' ]: for tag in e . data [ 'subj' ][ 'data' ][ 'entities' ][ 'annotations' ]: annotations . append ( tag [ 'normalized_text' ]) a_types . append ( tag [ 'type' ]) e . data [ 'subj' ][ 'processed' ][ 'hashtags' ] = hashtags e . data [ 'subj' ][ 'processed' ][ 'annotations' ] = annotations e . data [ 'subj' ][ 'processed' ][ 'a_types' ] = a_types context_a_domain = [] context_a_entity = [] e . data [ 'subj' ][ 'processed' ][ 'context_domain' ] = context_a_domain e . data [ 'subj' ][ 'processed' ][ 'context_entity' ] = context_a_entity # typeOfTweet (quote, retweet, tweet) if 'referenced_tweets' in e . data [ 'subj' ][ 'data' ] and len ( e . data [ 'subj' ][ 'data' ][ 'referenced_tweets' ]) > 0 \\ and 'type' in e . data [ 'subj' ][ 'data' ][ 'referenced_tweets' ][ 0 ]: e . data [ 'subj' ][ 'processed' ][ 'tweet_type' ] = e . data [ 'subj' ][ 'data' ][ 'referenced_tweets' ][ 0 ][ 'type' ] else : e . data [ 'subj' ][ 'processed' ][ 'tweet_type' ] = 'tweet' if e . data [ 'subj' ][ 'data' ][ 'conversation_id' ] == e . data [ 'subj' ][ 'pid' ]: logging . warning ( 'conversation id matches id -> tweet' ) # author processing author_data = TwitterWorker . get_author_data ( e . data [ 'subj' ][ 'data' ]) # should be always true? e . data [ 'subj' ][ 'processed' ][ 'location' ] = 'unknown' e . data [ 'subj' ][ 'processed' ][ 'followers' ] = 0 e . data [ 'subj' ][ 'processed' ][ 'bot_rating' ] = 1 if author_data : if 'location' in author_data : temp_location = geoencode ( author_data [ 'location' ]) if temp_location : e . data [ 'subj' ][ 'processed' ][ 'location' ] = temp_location e . data [ 'subj' ][ 'processed' ][ 'followers' ] = author_data [ 'public_metrics' ][ 'followers_count' ] e . data [ 'subj' ][ 'processed' ][ 'verified' ] = 10 if author_data [ 'verified' ] else 5 e . data [ 'subj' ][ 'processed' ][ 'name' ] = author_data [ 'username' ] if 'bot' not in author_data [ 'username' ] . lower () and 'bot' not in e . data [ 'subj' ][ 'data' ][ 'source' ]: e . data [ 'subj' ][ 'processed' ][ 'bot_rating' ] = 10 content_score = 1 text = e . data [ 'subj' ][ 'data' ][ 'text' ] . strip () . lower () if text and 'abstract' in e . data [ 'obj' ][ 'data' ] and 'lang' in e . data [ 'subj' ][ 'data' ]: spacy_result = self . spacy_process ( text , e . data [ 'obj' ][ 'data' ][ 'abstract' ], e . data [ 'subj' ][ 'data' ][ 'lang' ]) e . data [ 'subj' ][ 'processed' ][ 'words' ] = spacy_result [ 'common_words' ] e . data [ 'subj' ][ 'processed' ][ 'contains_abstract_raw' ] = spacy_result [ 'abstract' ] e . data [ 'subj' ][ 'processed' ][ 'contains_abstract' ] = self . normalize_abstract_value ( spacy_result [ 'abstract' ]) e . data [ 'subj' ][ 'processed' ][ 'sentiment_raw' ] = spacy_result [ 'sentiment' ] e . data [ 'subj' ][ 'processed' ][ 'sentiment' ] = self . normalize_sentiment_value ( spacy_result [ 'sentiment' ]) content_score = e . data [ 'subj' ][ 'processed' ][ 'contains_abstract' ] + e . data [ 'subj' ][ 'processed' ][ 'sentiment' ] content_score += score_length ( e . data [ 'subj' ][ 'processed' ][ 'length' ]) user_score = e . data [ 'subj' ][ 'processed' ][ 'bot_rating' ] if e . data [ 'subj' ][ 'processed' ][ 'followers' ] and type ( e . data [ 'subj' ][ 'processed' ][ 'followers' ]) == int \\ or type ( e . data [ 'subj' ][ 'processed' ][ 'followers' ]) == float : user_score += math . log ( e . data [ 'subj' ][ 'processed' ][ 'followers' ], 2 ) user_score += e . data [ 'subj' ][ 'processed' ][ 'verified' ] type_factor = score_type ( e . data [ 'subj' ][ 'processed' ][ 'tweet_type' ]) time_score = score_time ( e . data [ 'subj' ][ 'processed' ][ 'time_past' ]) logging . info ( 'score %s - %s - %s - %s ' % ( type_factor , time_score , user_score , content_score )) e . data [ 'subj' ][ 'processed' ][ 'time_score' ] = time_score e . data [ 'subj' ][ 'processed' ][ 'type_factor' ] = type_factor e . data [ 'subj' ][ 'processed' ][ 'user_score' ] = user_score e . data [ 'subj' ][ 'processed' ][ 'content_score' ] = content_score weights = { 'time' : 3 , 'user' : 6 , 'content' : 5 } e . data [ 'subj' ][ 'processed' ][ 'score' ] = weights [ 'time' ] * time_score e . data [ 'subj' ][ 'processed' ][ 'score' ] += weights [ 'user' ] * user_score e . data [ 'subj' ][ 'processed' ][ 'score' ] += weights [ 'content' ] * content_score e . data [ 'subj' ][ 'processed' ][ 'score' ] *= type_factor e . set ( 'state' , 'processed' ) self . dao . save_discussion_data ( e . data ) logging . warning ( 'publish ' + e . data [ 'obj' ][ 'data' ][ 'doi' ]) self . publish ( e ) else : logging . warning ( 'non twitter event' ) process_text_for_similarity ( nlp , text ) staticmethod process a given text after preparing it for language processing first remove certain words, and only allow verbs and nouns, no urls but annotations Parameters: Name Type Description Default nlp the language processing required text the text required Source code in src/twitter_worker.py @staticmethod def process_text_for_similarity ( nlp , text ): \"\"\"process a given text after preparing it for language processing first remove certain words, and only allow verbs and nouns, no urls but annotations Arguments: nlp: the language processing text: the text \"\"\" doc = nlp ( text ) if doc : return [ token . lemma_ . lower () for token in doc if not token . is_stop and not token . is_punct and not token . is_space and len ( token . lemma_ ) > 2 and not token . lemma_ . lower () . startswith ( 'http' ) and token . lemma_ . lower () != 'the' and token . lemma_ . lower () != 'amp' and token . lemma_ . lower () != 'and' and token . lemma_ . lower () != 'pos' and token . lemma_ . lower () != 'bull' and ( token . lemma_ . isalpha () or token . lemma_ . startswith ( '@' )) and ( token . pos_ == \"NOUN\" or token . pos_ == \"PROPN\" or token . pos_ == \"VERB\" )] return [] spacy_process ( self , text , abstract , lang ) process a tweet using spacy Parameters: Name Type Description Default text the tweet text required abstract the publication abstract required lang language of the tweet required Source code in src/twitter_worker.py def spacy_process ( self , text , abstract , lang ): \"\"\"process a tweet using spacy Arguments: text: the tweet text abstract: the publication abstract lang: language of the tweet \"\"\" result = { 'sentiment' : 0 , 'abstract' : 0 , 'common_words' : [] } if not text or not abstract or not lang : return result local_nlp = None # https://spacy.io/universe/project/spacy-langdetect # in case we have an undefined language supported = [ 'de' , 'es' , 'en' , 'fr' , 'ja' , 'it' , 'ru' , 'pl' ] if 'en' not in lang and lang in supported : if lang not in self . nlp or not self . nlp [ lang ]: self . nlp [ lang ] = spacy . load ( lang + '_core_news_md' ) self . nlp [ lang ] . add_pipe ( 'spacytextblob' ) local_nlp = self . nlp [ lang ] elif 'en' in lang : if not self . nlp [ 'en' ]: self . nlp [ 'en' ] = spacy . load ( 'en_core_web_md' ) self . nlp [ 'en' ] . add_pipe ( 'spacytextblob' ) local_nlp = self . nlp [ 'en' ] else : # neutral results if we have an unknown language logging . debug ( 'unknown language' ) return result # https://www.trinnovative.de/blog/2020-09-08-natural-language-processing-mit-spacy.html words = TwitterWorker . process_text_for_similarity ( local_nlp , text ) abstract_words = TwitterWorker . process_text_for_similarity ( local_nlp , abstract ) word_freq = Counter ( words ) sim = 0 tweet_doc = local_nlp ( \" \" . join ( words )) abstract_doc = local_nlp ( \" \" . join ( abstract_words )) if abstract_doc : sim = tweet_doc . similarity ( abstract_doc ) sentiment = local_nlp ( text ) . _ . polarity result = { 'sentiment' : sentiment , 'abstract' : sim , 'common_words' : word_freq . most_common ( 10 ) } return result start ( i = 0 ) staticmethod start the consumer Source code in src/twitter_worker.py @staticmethod def start ( i = 0 ): \"\"\"start the consumer \"\"\" esc = TwitterWorker ( i ) logging . warning ( TwitterWorker . log + 'Start %s ' % str ( i )) esc . consume () geoencode ( location ) geoencode a location Parameters: Name Type Description Default location the location we want a country for required Source code in src/twitter_worker.py @lru_cache ( maxsize = 50000 ) def geoencode ( location ): \"\"\"geoencode a location Arguments: location: the location we want a country for \"\"\" base_url = 'https://nominatim.openstreetmap.org/search?&format=jsonv2&addressdetails=1&q=' r = requests . get ( base_url + location ) if r . status_code == 200 : json_response = r . json () if json_response and isinstance ( json_response , list ) and len ( json_response ) > 0 : json_object = json_response [ 0 ] if 'address' in json_object and 'country_code' in json_object [ 'address' ]: return json_object [ 'address' ][ 'country_code' ] return None normalize ( string ) normalize a string Parameters: Name Type Description Default string the string to be normalized required Source code in src/twitter_worker.py def normalize ( string ): \"\"\"normalize a string Arguments: string: the string to be normalized \"\"\" return ( re . sub ( '[^a-zA-Z ]+' , '' , string )) . casefold () . strip () score_length ( length ) calculate a score based on a given length Parameters: Name Type Description Default length the length to base the score on required Source code in src/twitter_worker.py def score_length ( length ): \"\"\"calculate a score based on a given length Arguments: length: the length to base the score on \"\"\" if length < 50 : return 3 if length < 100 : return 6 return 10 score_time ( x ) calculate a score based on a given time in days Parameters: Name Type Description Default x the time to base the score on required Source code in src/twitter_worker.py def score_time ( x ): \"\"\"calculate a score based on a given time in days Arguments: x: the time to base the score on \"\"\" if x and type ( x ) == int or type ( x ) == float : y = 0 try : y = ( math . log ( x ) / math . log ( 1 / 7 ) + 3 ) * 10 except ValueError : logging . debug ( 'ValueError %s ' % str ( x )) if y > 30 : return 30 if y < 1 : return 1 return y return 1 score_type ( type ) calculate a score based on a given type Parameters: Name Type Description Default type the type to base the score on required Source code in src/twitter_worker.py def score_type ( type ): \"\"\"calculate a score based on a given type Arguments: type: the type to base the score on \"\"\" if type == 'quoted' : return 0.6 if type == 'replied_to' : return 0.7 if type == 'retweeted' : return 0.1 # original tweet return 1","title":"twitter worker"},{"location":"twitter_worker_ref/#twitter_worker.TwitterWorker","text":"process tweets","title":"TwitterWorker"},{"location":"twitter_worker_ref/#twitter_worker.TwitterWorker.get_author_data","text":"get the author data of a tweet Parameters: Name Type Description Default tweet_data the tweet data we want an author from required Source code in src/twitter_worker.py @staticmethod def get_author_data ( tweet_data ): \"\"\"get the author data of a tweet Arguments: tweet_data: the tweet data we want an author from \"\"\" author_id = tweet_data [ 'author_id' ] for user in tweet_data [ 'includes' ][ 'users' ]: if user [ 'id' ] == author_id : return user return None","title":"get_author_data()"},{"location":"twitter_worker_ref/#twitter_worker.TwitterWorker.normalize_abstract_value","text":"normalize the calculated value from an abstract comparison Parameters: Name Type Description Default value the value to be normalized required Source code in src/twitter_worker.py @staticmethod def normalize_abstract_value ( value ): \"\"\"normalize the calculated value from an abstract comparison Arguments: value: the value to be normalized \"\"\" if value > 0.9 : return 3 if value > 0.8 : return 5 if value > 0.5 : return 10 if value > 0.2 : return 3 return 1","title":"normalize_abstract_value()"},{"location":"twitter_worker_ref/#twitter_worker.TwitterWorker.normalize_sentiment_value","text":"normalize the calculated value from an sentiment comparison better sentiment -> better value Parameters: Name Type Description Default value the value to be normalized required Source code in src/twitter_worker.py @staticmethod def normalize_sentiment_value ( value ): \"\"\"normalize the calculated value from an sentiment comparison better sentiment -> better value Arguments: value: the value to be normalized \"\"\" if value > 0.6 : return 10 if value > 0.33 : return 9 if value > 0.1 : return 7 if value < - 0.6 : return 0 if value < - 0.33 : return 1 if value < - 0.1 : return 2 return 5","title":"normalize_sentiment_value()"},{"location":"twitter_worker_ref/#twitter_worker.TwitterWorker.on_message","text":"process a tweet Parameters: Name Type Description Default json_msg the json_msg containing the event to be processed required Source code in src/twitter_worker.py def on_message ( self , json_msg ): \"\"\"process a tweet Arguments: json_msg: the json_msg containing the event to be processed \"\"\" if not self . dao : self . dao = DAO () logging . warning ( self . log + \" create dao\" ) logging . info ( self . log + \"on message twitter consumer\" ) e = Event () e . from_json ( json_msg ) if e . get ( 'source_id' ) == 'twitter' : e . data [ 'subj' ][ 'processed' ] = {} e . data [ 'subj' ][ 'processed' ][ 'question_mark_count' ] = e . data [ 'subj' ][ 'data' ][ 'text' ] . count ( \"?\" ) e . data [ 'subj' ][ 'processed' ][ 'exclamation_mark_count' ] = e . data [ 'subj' ][ 'data' ][ 'text' ] . count ( \"!\" ) e . data [ 'subj' ][ 'processed' ][ 'length' ] = len ( e . data [ 'subj' ][ 'data' ][ 'text' ]) pub_timestamp = date ( 2012 , 1 , 1 ) if 'year' in e . data [ 'obj' ][ 'data' ]: pub_timestamp = date ( int ( e . data [ 'obj' ][ 'data' ][ 'year' ]), 1 , 1 ) if 'pub_date' in e . data [ 'obj' ][ 'data' ] and e . data [ 'obj' ][ 'data' ][ 'pub_date' ]: split_date = e . data [ 'obj' ][ 'data' ][ 'pub_date' ] . split ( '-' ) if len ( split_date ) > 2 : pub_timestamp = date ( int ( split_date [ 0 ]), int ( split_date [ 1 ]), int ( split_date [ 2 ])) e . data [ 'subj' ][ 'processed' ][ 'time_past' ] = ( date . today () - pub_timestamp ) . days hashtags = [] annotations = [] a_types = [] if 'entities' in e . data [ 'subj' ][ 'data' ]: if 'hashtags' in e . data [ 'subj' ][ 'data' ][ 'entities' ]: for tag in e . data [ 'subj' ][ 'data' ][ 'entities' ][ 'hashtags' ]: hashtags . append ( normalize ( tag [ 'tag' ])) if 'annotations' in e . data [ 'subj' ][ 'data' ][ 'entities' ]: for tag in e . data [ 'subj' ][ 'data' ][ 'entities' ][ 'annotations' ]: annotations . append ( tag [ 'normalized_text' ]) a_types . append ( tag [ 'type' ]) e . data [ 'subj' ][ 'processed' ][ 'hashtags' ] = hashtags e . data [ 'subj' ][ 'processed' ][ 'annotations' ] = annotations e . data [ 'subj' ][ 'processed' ][ 'a_types' ] = a_types context_a_domain = [] context_a_entity = [] e . data [ 'subj' ][ 'processed' ][ 'context_domain' ] = context_a_domain e . data [ 'subj' ][ 'processed' ][ 'context_entity' ] = context_a_entity # typeOfTweet (quote, retweet, tweet) if 'referenced_tweets' in e . data [ 'subj' ][ 'data' ] and len ( e . data [ 'subj' ][ 'data' ][ 'referenced_tweets' ]) > 0 \\ and 'type' in e . data [ 'subj' ][ 'data' ][ 'referenced_tweets' ][ 0 ]: e . data [ 'subj' ][ 'processed' ][ 'tweet_type' ] = e . data [ 'subj' ][ 'data' ][ 'referenced_tweets' ][ 0 ][ 'type' ] else : e . data [ 'subj' ][ 'processed' ][ 'tweet_type' ] = 'tweet' if e . data [ 'subj' ][ 'data' ][ 'conversation_id' ] == e . data [ 'subj' ][ 'pid' ]: logging . warning ( 'conversation id matches id -> tweet' ) # author processing author_data = TwitterWorker . get_author_data ( e . data [ 'subj' ][ 'data' ]) # should be always true? e . data [ 'subj' ][ 'processed' ][ 'location' ] = 'unknown' e . data [ 'subj' ][ 'processed' ][ 'followers' ] = 0 e . data [ 'subj' ][ 'processed' ][ 'bot_rating' ] = 1 if author_data : if 'location' in author_data : temp_location = geoencode ( author_data [ 'location' ]) if temp_location : e . data [ 'subj' ][ 'processed' ][ 'location' ] = temp_location e . data [ 'subj' ][ 'processed' ][ 'followers' ] = author_data [ 'public_metrics' ][ 'followers_count' ] e . data [ 'subj' ][ 'processed' ][ 'verified' ] = 10 if author_data [ 'verified' ] else 5 e . data [ 'subj' ][ 'processed' ][ 'name' ] = author_data [ 'username' ] if 'bot' not in author_data [ 'username' ] . lower () and 'bot' not in e . data [ 'subj' ][ 'data' ][ 'source' ]: e . data [ 'subj' ][ 'processed' ][ 'bot_rating' ] = 10 content_score = 1 text = e . data [ 'subj' ][ 'data' ][ 'text' ] . strip () . lower () if text and 'abstract' in e . data [ 'obj' ][ 'data' ] and 'lang' in e . data [ 'subj' ][ 'data' ]: spacy_result = self . spacy_process ( text , e . data [ 'obj' ][ 'data' ][ 'abstract' ], e . data [ 'subj' ][ 'data' ][ 'lang' ]) e . data [ 'subj' ][ 'processed' ][ 'words' ] = spacy_result [ 'common_words' ] e . data [ 'subj' ][ 'processed' ][ 'contains_abstract_raw' ] = spacy_result [ 'abstract' ] e . data [ 'subj' ][ 'processed' ][ 'contains_abstract' ] = self . normalize_abstract_value ( spacy_result [ 'abstract' ]) e . data [ 'subj' ][ 'processed' ][ 'sentiment_raw' ] = spacy_result [ 'sentiment' ] e . data [ 'subj' ][ 'processed' ][ 'sentiment' ] = self . normalize_sentiment_value ( spacy_result [ 'sentiment' ]) content_score = e . data [ 'subj' ][ 'processed' ][ 'contains_abstract' ] + e . data [ 'subj' ][ 'processed' ][ 'sentiment' ] content_score += score_length ( e . data [ 'subj' ][ 'processed' ][ 'length' ]) user_score = e . data [ 'subj' ][ 'processed' ][ 'bot_rating' ] if e . data [ 'subj' ][ 'processed' ][ 'followers' ] and type ( e . data [ 'subj' ][ 'processed' ][ 'followers' ]) == int \\ or type ( e . data [ 'subj' ][ 'processed' ][ 'followers' ]) == float : user_score += math . log ( e . data [ 'subj' ][ 'processed' ][ 'followers' ], 2 ) user_score += e . data [ 'subj' ][ 'processed' ][ 'verified' ] type_factor = score_type ( e . data [ 'subj' ][ 'processed' ][ 'tweet_type' ]) time_score = score_time ( e . data [ 'subj' ][ 'processed' ][ 'time_past' ]) logging . info ( 'score %s - %s - %s - %s ' % ( type_factor , time_score , user_score , content_score )) e . data [ 'subj' ][ 'processed' ][ 'time_score' ] = time_score e . data [ 'subj' ][ 'processed' ][ 'type_factor' ] = type_factor e . data [ 'subj' ][ 'processed' ][ 'user_score' ] = user_score e . data [ 'subj' ][ 'processed' ][ 'content_score' ] = content_score weights = { 'time' : 3 , 'user' : 6 , 'content' : 5 } e . data [ 'subj' ][ 'processed' ][ 'score' ] = weights [ 'time' ] * time_score e . data [ 'subj' ][ 'processed' ][ 'score' ] += weights [ 'user' ] * user_score e . data [ 'subj' ][ 'processed' ][ 'score' ] += weights [ 'content' ] * content_score e . data [ 'subj' ][ 'processed' ][ 'score' ] *= type_factor e . set ( 'state' , 'processed' ) self . dao . save_discussion_data ( e . data ) logging . warning ( 'publish ' + e . data [ 'obj' ][ 'data' ][ 'doi' ]) self . publish ( e ) else : logging . warning ( 'non twitter event' )","title":"on_message()"},{"location":"twitter_worker_ref/#twitter_worker.TwitterWorker.process_text_for_similarity","text":"process a given text after preparing it for language processing first remove certain words, and only allow verbs and nouns, no urls but annotations Parameters: Name Type Description Default nlp the language processing required text the text required Source code in src/twitter_worker.py @staticmethod def process_text_for_similarity ( nlp , text ): \"\"\"process a given text after preparing it for language processing first remove certain words, and only allow verbs and nouns, no urls but annotations Arguments: nlp: the language processing text: the text \"\"\" doc = nlp ( text ) if doc : return [ token . lemma_ . lower () for token in doc if not token . is_stop and not token . is_punct and not token . is_space and len ( token . lemma_ ) > 2 and not token . lemma_ . lower () . startswith ( 'http' ) and token . lemma_ . lower () != 'the' and token . lemma_ . lower () != 'amp' and token . lemma_ . lower () != 'and' and token . lemma_ . lower () != 'pos' and token . lemma_ . lower () != 'bull' and ( token . lemma_ . isalpha () or token . lemma_ . startswith ( '@' )) and ( token . pos_ == \"NOUN\" or token . pos_ == \"PROPN\" or token . pos_ == \"VERB\" )] return []","title":"process_text_for_similarity()"},{"location":"twitter_worker_ref/#twitter_worker.TwitterWorker.spacy_process","text":"process a tweet using spacy Parameters: Name Type Description Default text the tweet text required abstract the publication abstract required lang language of the tweet required Source code in src/twitter_worker.py def spacy_process ( self , text , abstract , lang ): \"\"\"process a tweet using spacy Arguments: text: the tweet text abstract: the publication abstract lang: language of the tweet \"\"\" result = { 'sentiment' : 0 , 'abstract' : 0 , 'common_words' : [] } if not text or not abstract or not lang : return result local_nlp = None # https://spacy.io/universe/project/spacy-langdetect # in case we have an undefined language supported = [ 'de' , 'es' , 'en' , 'fr' , 'ja' , 'it' , 'ru' , 'pl' ] if 'en' not in lang and lang in supported : if lang not in self . nlp or not self . nlp [ lang ]: self . nlp [ lang ] = spacy . load ( lang + '_core_news_md' ) self . nlp [ lang ] . add_pipe ( 'spacytextblob' ) local_nlp = self . nlp [ lang ] elif 'en' in lang : if not self . nlp [ 'en' ]: self . nlp [ 'en' ] = spacy . load ( 'en_core_web_md' ) self . nlp [ 'en' ] . add_pipe ( 'spacytextblob' ) local_nlp = self . nlp [ 'en' ] else : # neutral results if we have an unknown language logging . debug ( 'unknown language' ) return result # https://www.trinnovative.de/blog/2020-09-08-natural-language-processing-mit-spacy.html words = TwitterWorker . process_text_for_similarity ( local_nlp , text ) abstract_words = TwitterWorker . process_text_for_similarity ( local_nlp , abstract ) word_freq = Counter ( words ) sim = 0 tweet_doc = local_nlp ( \" \" . join ( words )) abstract_doc = local_nlp ( \" \" . join ( abstract_words )) if abstract_doc : sim = tweet_doc . similarity ( abstract_doc ) sentiment = local_nlp ( text ) . _ . polarity result = { 'sentiment' : sentiment , 'abstract' : sim , 'common_words' : word_freq . most_common ( 10 ) } return result","title":"spacy_process()"},{"location":"twitter_worker_ref/#twitter_worker.TwitterWorker.start","text":"start the consumer Source code in src/twitter_worker.py @staticmethod def start ( i = 0 ): \"\"\"start the consumer \"\"\" esc = TwitterWorker ( i ) logging . warning ( TwitterWorker . log + 'Start %s ' % str ( i )) esc . consume ()","title":"start()"},{"location":"twitter_worker_ref/#twitter_worker.geoencode","text":"geoencode a location Parameters: Name Type Description Default location the location we want a country for required Source code in src/twitter_worker.py @lru_cache ( maxsize = 50000 ) def geoencode ( location ): \"\"\"geoencode a location Arguments: location: the location we want a country for \"\"\" base_url = 'https://nominatim.openstreetmap.org/search?&format=jsonv2&addressdetails=1&q=' r = requests . get ( base_url + location ) if r . status_code == 200 : json_response = r . json () if json_response and isinstance ( json_response , list ) and len ( json_response ) > 0 : json_object = json_response [ 0 ] if 'address' in json_object and 'country_code' in json_object [ 'address' ]: return json_object [ 'address' ][ 'country_code' ] return None","title":"geoencode()"},{"location":"twitter_worker_ref/#twitter_worker.normalize","text":"normalize a string Parameters: Name Type Description Default string the string to be normalized required Source code in src/twitter_worker.py def normalize ( string ): \"\"\"normalize a string Arguments: string: the string to be normalized \"\"\" return ( re . sub ( '[^a-zA-Z ]+' , '' , string )) . casefold () . strip ()","title":"normalize()"},{"location":"twitter_worker_ref/#twitter_worker.score_length","text":"calculate a score based on a given length Parameters: Name Type Description Default length the length to base the score on required Source code in src/twitter_worker.py def score_length ( length ): \"\"\"calculate a score based on a given length Arguments: length: the length to base the score on \"\"\" if length < 50 : return 3 if length < 100 : return 6 return 10","title":"score_length()"},{"location":"twitter_worker_ref/#twitter_worker.score_time","text":"calculate a score based on a given time in days Parameters: Name Type Description Default x the time to base the score on required Source code in src/twitter_worker.py def score_time ( x ): \"\"\"calculate a score based on a given time in days Arguments: x: the time to base the score on \"\"\" if x and type ( x ) == int or type ( x ) == float : y = 0 try : y = ( math . log ( x ) / math . log ( 1 / 7 ) + 3 ) * 10 except ValueError : logging . debug ( 'ValueError %s ' % str ( x )) if y > 30 : return 30 if y < 1 : return 1 return y return 1","title":"score_time()"},{"location":"twitter_worker_ref/#twitter_worker.score_type","text":"calculate a score based on a given type Parameters: Name Type Description Default type the type to base the score on required Source code in src/twitter_worker.py def score_type ( type ): \"\"\"calculate a score based on a given type Arguments: type: the type to base the score on \"\"\" if type == 'quoted' : return 0.6 if type == 'replied_to' : return 0.7 if type == 'retweeted' : return 0.1 # original tweet return 1","title":"score_type()"}]}